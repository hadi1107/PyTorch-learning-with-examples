# 预备知识
## tensor基本操作
- 张量即多维数组，相较于ndarray而言，张量支持GPU加速和自动微分
- 张量的shape属性是其形状，即沿每个轴的长度，size是张量中元素的总数，利用numel()方法得到
- reshape()方法用于改变张量形状，当某个轴的参数指定为-1时，表示自动计算该轴大小
- 轴的规律：数字最大的轴即为shape的最右一个参数，这个轴对应的是一个元素为值的list
- x+y，x-y，x * y，x/y，x ** y，exp(x)，x==y 等计算方法，都是”按元素“方式
- cat()方法用于将张量连结(concatenate)在一起，按哪个轴cat，哪个轴的长度就会增加
- 广播机制的工作方式为两步：
  - 通过适当复制元素扩展张量，使得两个张量具有相同的形状
  - 对生成的新张量执行按元素操作
- 执行一定操作后，内存是否原地更新：可以通过id()方法实验：
  - y = y + x False
  - y += x True
  - y[ : ] = x + y True
***
## 数据预处理
- ”NaN“项表示缺失值，为了处理缺失的数据，典型的方法包括插值法和删除法
- `inputs = inputs.fillna(inputs.mean(numeric_only=True))`fillna()方法会将DataFrame中的缺失值替换为指定的值，而在这里，指定的值是数值列的平均值。numeric_only=True参数用于指定只计算数值列的平均值，而忽略其他非数值列
***
## 线性代数
- 矩阵的转置可以直接通过A.T得到
- 两个矩阵的“按元素”乘法称为哈达玛积(Hadamard product)
- 张量的降维操作：通过指定sum(axis = dim_name)方法的axis，可以只对某个轴进行求和，求和后，该轴的维数在输出张量中消失
- 非降维的求和方式，在sum()方法中指定keepdims = True，可以保持维数不变
- 矩阵向量积的表示方法为torch.mv(A,x),A的列数必须与x的维数相同，矩阵乘法为torch.mm(A,B)
- 范数(norm)是一种将向量映射到标量的函数f，可以反映向量有多”大“，欧几里得距离即L2范数，数值上即向量元素平方和的平方根。L1范数为向量元素的绝对值之和。以上两种范数分别用torch.norm()和torch.abs().sum()得到
***
## 自动微分
自动微分的基本思想是根据链式法则，将函数的导数计算表示为计算图上的反向传播。在PyTorch中，自动微分是通过torch.autograd模块来实现的。这个模块中的核心类是Variable,它是一种特殊的Tensor对象，包含了该张量对应的计算图中的节点信息。当我们执行一些操作时（比如张量的加减乘除、卷积、池化等），PyTorch会自动创建一个计算图，其中每个操作都表示为图上的节点。在这个过程中，每个操作都会生成一个Variable对象，并将其与计算图上的节点关联起来。当我们执行反向传播时，PyTorch会从计算图的最终结果开始，计算每个节点对输入的梯度，并将其传播回每个节点的输入。
***
